{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b443153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from utils import parse_filelist\n",
    "from text import cmudict\n",
    "\n",
    "data_dir = Path.cwd() / \"../LJ_samples\"\n",
    "splits_dir = Path.cwd() / \"resources/filelists/ljspeech\"\n",
    "cmudict_path = 'resources/cmu_dictionary'\n",
    "\n",
    "dictionary = cmudict.CMUDict(cmudict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f22bd",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Create new filelists (arpabet convertible samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "9991ac9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: LJ001-0007, idx: 6\n",
      "Original: the earliest book printed with movable types, the Gutenberg, or \"forty-two line Bible\" of about 1455,\n",
      "Normalized: the earliest book printed with movable types, the Gutenberg, or \"forty-two line Bible\" of about fourteen fifty-five,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>norm_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LJ001-0001</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LJ001-0002</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LJ001-0003</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LJ001-0004</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LJ001-0005</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                         transcript  \\\n",
       "0  LJ001-0001  Printing, in the only sense with which we are ...   \n",
       "1  LJ001-0002                     in being comparatively modern.   \n",
       "2  LJ001-0003  For although the Chinese took impressions from...   \n",
       "3  LJ001-0004  produced the block books, which were the immed...   \n",
       "4  LJ001-0005  the invention of movable metal letters in the ...   \n",
       "\n",
       "                                     norm_transcript  \n",
       "0  Printing, in the only sense with which we are ...  \n",
       "1                     in being comparatively modern.  \n",
       "2  For although the Chinese took impressions from...  \n",
       "3  produced the block books, which were the immed...  \n",
       "4  the invention of movable metal letters in the ...  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#metadat.csv\n",
    "filepaths_and_text = parse_filelist(data_dir / \"metadata.csv\", split_char='|')\n",
    "df = pd.DataFrame(np.array(filepaths_and_text), columns=[\"id\", \"transcript\", \"norm_transcript\"])\n",
    "for idx in range(10):\n",
    "    id = df.iloc[idx][\"id\"]\n",
    "    if df.loc[idx, \"transcript\"] != df.loc[idx, \"norm_transcript\"]:\n",
    "        print(f\"ID: {id}, idx: {idx}\")\n",
    "        print(f\"Original: {df.loc[idx, 'transcript']}\")\n",
    "        print(f\"Normalized: {df.loc[idx, 'norm_transcript']}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "fb69711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits 'text' is metadata 'norm_transcript':  True\n",
      "Train samples: 11947, Validation samples: 95, Test samples: 488\n",
      "Train ratio: 0.953, Validation ratio: 0.008, Test ratio: 0.039\n"
     ]
    }
   ],
   "source": [
    "def get_split_df(filename:str=\"train.txt\"):\n",
    "    filepaths_and_text = parse_filelist(splits_dir / filename, split_char='|')\n",
    "    split_df = pd.DataFrame(np.array(filepaths_and_text), columns=[\"id\", \"text\"])\n",
    "    split_df[\"id\"] = split_df[\"id\"].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
    "    return split_df\n",
    "\n",
    "train_df = get_split_df(\"train.txt\")\n",
    "valid_df = get_split_df(\"valid.txt\")\n",
    "test_df = get_split_df(\"test.txt\")\n",
    "\n",
    "_ = train_df.merge(df, on=\"id\", how=\"left\")\n",
    "print(\"splits 'text' is metadata 'norm_transcript': \", np.all(_[\"norm_transcript\"] == _[\"text\"]))\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Validation samples: {len(valid_df)}, Test samples: {len(test_df)}\")\n",
    "print(f\"Train ratio: {len(train_df) / (len(train_df) + len(valid_df) + len(test_df)):.3f}, \"\n",
    "      f\"Validation ratio: {len(valid_df) / (len(train_df) + len(valid_df) + len(test_df)):.3f}, \"\n",
    "      f\"Test ratio: {len(test_df) / (len(train_df) + len(valid_df) + len(test_df)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f67e25c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train good samples: 9892,     Train conversion rate: 0.828\n",
      "Validation good samples: 76,     Validation conversion rate: 0.800\n",
      "Test good samples: 398,     Test conversion rate: 0.816\n",
      "Train ratio: 0.954, Validation ratio: 0.007, Test ratio: 0.038\n"
     ]
    }
   ],
   "source": [
    "def good_bad_df(split_df):\n",
    "    \"\"\"\n",
    "    Transcribe the samples from the dataframe to ARPabet\n",
    "    and return two dataframes:\n",
    "    1. good_samples_df: samples with valid ARPAbet\n",
    "    2. bad_samples_df: samples with invalid ARPAbet\n",
    "    \"\"\"\n",
    "    good_samples = []\n",
    "    bad_samples = []\n",
    "    for idx in split_df.index:\n",
    "        id = split_df.loc[idx][\"id\"]\n",
    "        text = split_df.loc[idx, \"text\"]\n",
    "        #text = \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n",
    "        cleaner_names=[\"english_cleaners_v2\"]\n",
    "        arpabets = text_to_arpabet(text, dictionary, cleaner_names)\n",
    "        arpabets = check_arpabet(arpabets, remove_punctuation=True)\n",
    "        if arpabets is None:\n",
    "            bad_samples.append({\"id\": id,\n",
    "                                \"text\": text,\n",
    "                                \"arpabets\": arpabets})\n",
    "        else:\n",
    "            good_samples.append({\"id\": id,\n",
    "                                \"text\": text,\n",
    "                                \"arpabets\": arpabets})\n",
    "    good_samples_df = pd.DataFrame(good_samples)\n",
    "    bad_samples_df = pd.DataFrame(bad_samples)\n",
    "    return good_samples_df, bad_samples_df\n",
    "\n",
    "train_good_df, train_bad_df = good_bad_df(train_df)\n",
    "valid_good_df, valid_bad_df = good_bad_df(valid_df)\n",
    "test_good_df, test_bad_df = good_bad_df(test_df)\n",
    "print(f\"Train good samples: {len(train_good_df)}, \\\n",
    "    Train conversion rate: {len(train_good_df) / len(train_df):.3f}\")\n",
    "print(f\"Validation good samples: {len(valid_good_df)}, \\\n",
    "    Validation conversion rate: {len(valid_good_df) / len(valid_df):.3f}\")\n",
    "print(f\"Test good samples: {len(test_good_df)}, \\\n",
    "    Test conversion rate: {len(test_good_df) / len(test_df):.3f}\")\n",
    "\n",
    "n_valid = (len(train_good_df) + len(valid_good_df) + len(test_good_df))\n",
    "print(f\"Train ratio: {len(train_good_df) / n_valid:.3f}, \"\n",
    "      f\"Validation ratio: {len(valid_good_df) / n_valid:.3f}, \"\n",
    "      f\"Test ratio: {len(test_good_df) / n_valid:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "172a462d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>arpabets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LJ050-0234</td>\n",
       "      <td>It has used other Treasury law enforcement age...</td>\n",
       "      <td>[{IH1 T}, {HH AE1 Z}, {Y UW1 Z D}, {AH1 DH ER0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LJ050-0207</td>\n",
       "      <td>Although Chief Rowley does not complain about ...</td>\n",
       "      <td>[{AO2 L DH OW1}, {CH IY1 F}, {R OW1 L IY0}, {D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LJ048-0203</td>\n",
       "      <td>The three officers confirm that their primary ...</td>\n",
       "      <td>[{DH AH0}, {TH R IY1}, {AO1 F AH0 S ER0 Z}, {K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LJ003-0182</td>\n",
       "      <td>The tried and the untried, young and old, were...</td>\n",
       "      <td>[{DH AH0}, {T R AY1 D}, {AH0 N D}, {DH AH0}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LJ044-0166</td>\n",
       "      <td>According to Marina Oswald, he thought that wo...</td>\n",
       "      <td>[{AH0 K AO1 R D IH0 NG}, {T UW1}, {M ER0 IY1 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LJ019-0208</td>\n",
       "      <td>The proposal made was to purchase some fifty t...</td>\n",
       "      <td>[{DH AH0}, {P R AH0 P OW1 Z AH0 L}, {M EY1 D},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LJ021-0146</td>\n",
       "      <td>I shall seek assurances of the making and main...</td>\n",
       "      <td>[{AY1}, {SH AE1 L}, {S IY1 K}, {AH0 SH UH1 R A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LJ014-0083</td>\n",
       "      <td>which, having possessed herself of the murdere...</td>\n",
       "      <td>[{W IH1 CH}, {HH AE1 V IH0 NG}, {P AH0 Z EH1 S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LJ035-0121</td>\n",
       "      <td>This is the period during which Oswald would h...</td>\n",
       "      <td>[{DH IH1 S}, {IH1 Z}, {DH AH0}, {P IH1 R IY0 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LJ049-0118</td>\n",
       "      <td>Enactment of this statute would mean that the ...</td>\n",
       "      <td>[{EH0 N AE1 K T M AH0 N T}, {AH1 V}, {DH IH1 S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0  LJ050-0234  It has used other Treasury law enforcement age...   \n",
       "1  LJ050-0207  Although Chief Rowley does not complain about ...   \n",
       "2  LJ048-0203  The three officers confirm that their primary ...   \n",
       "3  LJ003-0182  The tried and the untried, young and old, were...   \n",
       "4  LJ044-0166  According to Marina Oswald, he thought that wo...   \n",
       "5  LJ019-0208  The proposal made was to purchase some fifty t...   \n",
       "6  LJ021-0146  I shall seek assurances of the making and main...   \n",
       "7  LJ014-0083  which, having possessed herself of the murdere...   \n",
       "8  LJ035-0121  This is the period during which Oswald would h...   \n",
       "9  LJ049-0118  Enactment of this statute would mean that the ...   \n",
       "\n",
       "                                            arpabets  \n",
       "0  [{IH1 T}, {HH AE1 Z}, {Y UW1 Z D}, {AH1 DH ER0...  \n",
       "1  [{AO2 L DH OW1}, {CH IY1 F}, {R OW1 L IY0}, {D...  \n",
       "2  [{DH AH0}, {TH R IY1}, {AO1 F AH0 S ER0 Z}, {K...  \n",
       "3  [{DH AH0}, {T R AY1 D}, {AH0 N D}, {DH AH0}, {...  \n",
       "4  [{AH0 K AO1 R D IH0 NG}, {T UW1}, {M ER0 IY1 N...  \n",
       "5  [{DH AH0}, {P R AH0 P OW1 Z AH0 L}, {M EY1 D},...  \n",
       "6  [{AY1}, {SH AE1 L}, {S IY1 K}, {AH0 SH UH1 R A...  \n",
       "7  [{W IH1 CH}, {HH AE1 V IH0 NG}, {P AH0 Z EH1 S...  \n",
       "8  [{DH IH1 S}, {IH1 Z}, {DH AH0}, {P IH1 R IY0 A...  \n",
       "9  [{EH0 N AE1 K T M AH0 N T}, {AH1 V}, {DH IH1 S...  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_good_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "bed4ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_split_file(filepath, splits_df):\n",
    "    lines = []\n",
    "    for row in splits_df[[\"id\", \"text\"]].values:\n",
    "        id = row[0]\n",
    "        text = row[1]\n",
    "        line = f\"DUMMY/{id}.wav|{text}\\n\"\n",
    "        lines.append(line)\n",
    "    with open(filepath, \"w\") as file:\n",
    "        file.writelines(lines)\n",
    "    print(f\"Filelist written to {filepath}\")\n",
    "\n",
    "\n",
    "#write_split_file(splits_dir / \"train_v0.txt\", train_good_df)\n",
    "#write_split_file(splits_dir / \"valid_v0.txt\", valid_good_df)\n",
    "#write_split_file(splits_dir / \"test_v0.txt\", test_good_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc15711",
   "metadata": {},
   "source": [
    "# ARPabet to IPA ternary traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "37902f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of traits: 24 Emb dim: 25\n"
     ]
    }
   ],
   "source": [
    "train_df = get_split_df(\"train_v0.txt\")\n",
    "valid_df = get_split_df(\"valid_v0.txt\")\n",
    "test_df = get_split_df(\"test_v0.txt\")\n",
    "\n",
    "traits_list = [\"syl\", \"son\", \"cons\", \"cont\", \"delrel\", \"lat\",\n",
    "                \"nas\", \"strid\", \"voi\", \"sg\", \"cg\", \"ant\", \"cor\",\n",
    "                \"distr\", \"lab\", \"hi\", \"lo\", \"back\", \"round\",\n",
    "                \"velaric\", \"tense\", \"long\", \"hitone\", \"hireg\"]\n",
    "N_traits = len(traits_list)\n",
    "emb_dim = N_traits + 1    # add a dim for one hot punctuation/pause token\n",
    "space_tok = np.zeros((1,emb_dim))\n",
    "space_tok[0,-1] = 1\n",
    "print(f\"Number of traits: {N_traits} Emb dim: {emb_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9c9e92c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 'LJ001-0001'\n",
    "text = train_df.loc[train_df[\"id\"] == id, \"text\"].values[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from text.cmudict import CMUDict\n",
    "from text import _clean_text\n",
    "from text import _punctuation_list\n",
    "\n",
    "import re\n",
    "\n",
    "_curly_re = re.compile(r\"(.*?)\\{(.+?)\\}(.*)\")\n",
    "_composed_re = re.compile(r\"\\b[a-zA-Z]+(?:-[a-zA-Z]+)+\\b\")  # composed words with dashes\n",
    "\n",
    "def text_to_ipa(\n",
    "    text: str,\n",
    "    dictionary: Optional[CMUDict] = None,\n",
    "    cleaner_names: List[str] = [\"english_cleaners_v2\"],\n",
    "    remove_punctuation: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Convert text to IPA characters sequence.\n",
    "    \"\"\"\n",
    "    arp_list = text_to_arpabet(text, dictionary, cleaner_names)\n",
    "    arp_list = check_arpabet(arp_list, remove_punctuation=remove_punctuation)\n",
    "    if arp_list is None:\n",
    "        print(f\"Unable to convert to ARPAbet : {text}\")\n",
    "        return None\n",
    "    else:\n",
    "        # convert ARPAbet to IPA\n",
    "        ipawords_list = [get_ipa_from_arp(w) for w in arp_list]\n",
    "        return ipawords_list\n",
    "\n",
    "def ipa_to_ternary(\n",
    "    ipawords_list: List[str],\n",
    ")-> np.ndarray:\n",
    "    ternary_seq = []\n",
    "    for word_ipa in ipawords_list:\n",
    "        if ft.validate_word(word_ipa):\n",
    "            emb_arr = ft.word_array(traits_list, word_ipa) #shape: (n_chars, N_traits)\n",
    "            ternary_seq.append(np.pad(emb_arr, ((0, 0), (0, 1)), mode='constant', constant_values=0))\n",
    "        elif word_ipa in _punctuation_list:\n",
    "            ternary_seq.append(space_tok)\n",
    "        else:\n",
    "            print(f\"Word not found in panphon: {word_ipa}\")\n",
    "            continue\n",
    "    return np.concatenate(ternary_seq, axis=0)\n",
    "\n",
    "############################\n",
    "## text to ARPAbet functions\n",
    "############################\n",
    "\n",
    "def get_arpabet_dash(\n",
    "    word: str,\n",
    "    dictionary: Optional[CMUDict] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get ARPAbet transcription for a word, handling dashed composed words.\n",
    "    More specifically, if the word contains a dash, and is not in the dictionary,\n",
    "    split the word at the dash and get ARPAbet for each part.\n",
    "    The parts are then joined with a space\n",
    "    \"\"\"\n",
    "    word_arpabet = dictionary.lookup(word)\n",
    "    if word_arpabet is not None:\n",
    "        return \"{\" + word_arpabet[0] + \"}\"\n",
    "    elif _composed_re.match(word):\n",
    "        words = word.split(\"-\")\n",
    "        words_arpabet = [get_arpabet_dash(w, dictionary) for w in words]\n",
    "        return \" \".join(words_arpabet)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def text_to_arpabet(\n",
    "    text: str,\n",
    "    dictionary: Optional[CMUDict] = None,\n",
    "    cleaner_names: List[str] = [\"english_cleaners_v2\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert text to ARPAbet words list.\n",
    "\n",
    "    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n",
    "    in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n",
    "\n",
    "    Args:\n",
    "        text: input text\n",
    "        dictionary: CMU dictionary\n",
    "        cleaner_names: list of cleaner names\n",
    "    Returns:\n",
    "        ARPAbet words list (List[{\"ARP1 ARP2 ...ARPN}\" or \"PUNC\"])\n",
    "    \"\"\"\n",
    "    arp_words = []\n",
    "    # Check for curly braces and treat their contents as ARPAbet:\n",
    "    while len(text):\n",
    "        m = _curly_re.match(text)\n",
    "        if not m:\n",
    "            clean_text = _clean_text(text, cleaner_names)\n",
    "            clean_text = [\n",
    "                get_arpabet_dash(w, dictionary) for w in clean_text.split(\" \")\n",
    "            ]\n",
    "            arp_words += clean_text\n",
    "            break\n",
    "        else:\n",
    "            arp_words += text_to_arpabet(m.group(1), dictionary, cleaner_names)\n",
    "            arp_words += [\"{\" + m.group(2) + \"}\"]\n",
    "            text = m.group(3)\n",
    "    return arp_words\n",
    "\n",
    "\n",
    "def check_arpabet(\n",
    "    arp_words: List[str],\n",
    "    remove_punctuation: bool = False,\n",
    ") -> List[str] | None:\n",
    "    \"\"\"\n",
    "    Check if all words are ARPabet encoded (or punctuation).\n",
    "    If not, return None.\n",
    "\n",
    "    Args:\n",
    "        arp_words: list of words\n",
    "        remove_punctuation: if True, remove punctuation from the list\n",
    "    Returns:\n",
    "        list of words (\"{ARP1 ARP2 ...ARPN}\", or \"PUNC\") if all are valid\n",
    "        None otherwise\n",
    "    \"\"\"\n",
    "    mask_arp = [elem.startswith(\"{\") and elem.endswith(\"}\") for elem in arp_words]\n",
    "    mask_punc = [elem in _punctuation_list for elem in arp_words]\n",
    "    mask_invalid = [not (arp or punct) for arp, punct in zip(mask_arp, mask_punc)]\n",
    "    if any(mask_invalid):\n",
    "        return None\n",
    "    elif remove_punctuation:\n",
    "        return [elem for elem in arp_words if elem not in _punctuation_list]\n",
    "    else:\n",
    "        return arp_words\n",
    "\n",
    "###########################\n",
    "#### ARPAbet to IPA functions\n",
    "###########################\n",
    "\n",
    "def get_ipa_from_arp(\n",
    "        arp_seq: str\n",
    "        )-> str | None:\n",
    "    \"\"\"\n",
    "    Get IPA transcription for an ARPabet sequence (format \"{ARP1 ARP2 ...ARPN}\").\n",
    "    Handles punctuation words as well (\".\", \",\", ...) by returning them as is.\n",
    "    SHOULD BE CALLED AFTER check_arpabet() to ensure the ARPAbet sequence is valid.\n",
    "    If the ARPAbet sequence is not valid, return None.\n",
    "\n",
    "    Args:\n",
    "        arp_seq: ARPAbet sequence or punctuation string\n",
    "    Returns:\n",
    "        IPA transcription : str or None if not found\n",
    "                            ex : \"pɹɪntɪŋ\"\n",
    "    \"\"\"\n",
    "    def arpchar_to_ipa(arp: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Get IPA transcription for an ARPAbet character.\n",
    "        Try to find the original ARPAbet character. If not found,\n",
    "        fallback to the ARPAbet character without stress markers\n",
    "        \"\"\"\n",
    "        if arp in arpabet2ipa:\n",
    "            return arpabet2ipa[arp]\n",
    "        else:\n",
    "            arp = arp.replace(\"1\", \"\").replace(\"2\", \"\").replace(\"0\", \"\")\n",
    "            return arpabet2ipa[arp]\n",
    "    \n",
    "    if arp_seq.startswith(\"{\") and arp_seq.endswith(\"}\"):\n",
    "        arp_seq = arp_seq[1:-1].split(\" \")\n",
    "        ipa_seq = [arpchar_to_ipa(arp) for arp in arp_seq]\n",
    "        return \"\".join(ipa_seq)\n",
    "    elif arp_seq in _punctuation_list:\n",
    "        return arp_seq\n",
    "    else:\n",
    "        print(\"Invalid ARPAbet sequence, should be checked with check_arpabet()\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "9f5f49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pɹɪntɪŋ', 'ɪn', 'ðə', 'oʊnli', 'sɛns', 'wɪð', 'wɪtʃ', 'wi', 'ɑɹ', 'æt', 'pɹɛzənt', 'kənsɜ˞nd', 'dɪfə˞z', 'fɹʌm', 'moʊst', 'ɪf', 'nɑt', 'fɹʌm', 'ɔl', 'ðə', 'ɑɹts', 'ənd', 'kɹæfts', 'ɹɛpɹəzɛntəd', 'ɪn', 'ðə', 'ɛksəbɪʃən']\n"
     ]
    }
   ],
   "source": [
    "ipawords_list = text_to_ipa(text, dictionary, cleaner_names=[\"english_cleaners_v2\"], remove_punctuation=False)\n",
    "ipawords_list_ = text_to_ipa(text, dictionary, cleaner_names=[\"english_cleaners_v2\"], remove_punctuation=True)\n",
    "print(ipawords_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02473925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panphon\n",
    "from utils import intersperse\n",
    "\n",
    "ft = panphon.FeatureTable()\n",
    "\n",
    "def ipa_to_ternary(\n",
    "    ipawords_list: List[str],\n",
    ")-> np.ndarray:\n",
    "    ternary_seq = []\n",
    "    for word_ipa in ipawords_list:\n",
    "        if ft.validate_word(word_ipa):\n",
    "            emb_arr = ft.word_array(traits_list, word_ipa) #shape: (n_chars, N_traits)\n",
    "            ternary_seq.append(np.pad(emb_arr, ((0, 0), (0, 1)), mode='constant', constant_values=0))\n",
    "        elif word_ipa in _punctuation_list:\n",
    "            ternary_seq.append(space_tok)\n",
    "        else:\n",
    "            print(f\"Word not found in panphon: {word_ipa}\")\n",
    "            continue\n",
    "    return np.concatenate(ternary_seq, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "32166e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to convert to ARPAbet : qv dfs\n"
     ]
    }
   ],
   "source": [
    "ipawords_list = text_to_ipa(\"qv dfs\", dictionary, cleaner_names=[\"english_cleaners_v2\"], remove_punctuation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "5f82c374",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[350]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m caca = \u001b[43mipa_to_ternary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mipawords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m pipi = ipa_to_ternary(intersperse(ipawords_list, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[342]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mipa_to_ternary\u001b[39m\u001b[34m(ipawords_list)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mipa_to_ternary\u001b[39m(\n\u001b[32m      7\u001b[39m     ipawords_list: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m      8\u001b[39m )-> np.ndarray:\n\u001b[32m      9\u001b[39m     ternary_seq = []\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_ipa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mipawords_list\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_ipa\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43memb_arr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraits_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_ipa\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#shape: (n_chars, N_traits)\u001b[39;49;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "caca = ipa_to_ternary(ipawords_list)\n",
    "pipi = ipa_to_ternary(intersperse(ipawords_list, \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "110a5801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((113, 25), (143, 25))"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caca.shape, pipi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1302d22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1.,  1., ...,  0.,  0.,  0.],\n",
       "       [-1.,  1., -1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1., -1., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [-1., -1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1., -1., ...,  0.,  0.,  0.],\n",
       "       [-1.,  1.,  1., ...,  0.,  0.,  0.]], shape=(113, 25))"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ternary_seq = []\n",
    "for word_ipa in ipawords_list:\n",
    "    if ft.validate_word(word_ipa):\n",
    "        emb_arr = ft.word_array(traits_list, word_ipa) #shape: (n_chars, N_traits)\n",
    "        ternary_seq.append(np.pad(emb_arr, ((0, 0), (0, 1)), mode='constant', constant_values=0))\n",
    "    elif word_ipa in _punctuation_list:\n",
    "        ternary_seq.append(space_tok)\n",
    "    else:\n",
    "        print(f\"Word not found in panphon: {word_ipa}\")\n",
    "        #ternary_seq.append(space_tok)\n",
    "        #ternary_seq.append(np.zeros((1, emb_dim)))\n",
    "        continue\n",
    "np.concatenate(ternary_seq, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import panphon\n",
    "from utils import intersperse\n",
    "from text.cleaners import _punctuation_list\n",
    "from text.arpabet import arpabet2ipa\n",
    "\n",
    "ft = panphon.FeatureTable()\n",
    "\n",
    "#'ɚ' and 'ɝ' were not recognized by panphon, we replaced them with 'ɜ˞' and 'ə˞' respectively\n",
    "# in the arpabet2ipa dictionary\n",
    "print(ft.validate_word('ɚ'))\n",
    "print(ft.validate_word('ɝ'))\n",
    "print(ft.validate_word('ɜ˞'))\n",
    "print(ft.validate_word('ə˞'))\n",
    "\n",
    "for k, v in arpabet2ipa.items():\n",
    "    if not ft.validate_word(v):\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7aa2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_seq = []\n",
    "for word_ipa in ipa_seq:\n",
    "    if ft.validate_word(word_ipa):\n",
    "        emb_arr = ft.word_array(traits_list, word_ipa) #shape: (n_chars, N_traits)\n",
    "        ternary_seq.append(np.pad(emb_arr, ((0, 0), (0, 1)), mode='constant', constant_values=0))\n",
    "    elif word_ipa in _punctuation_list:\n",
    "        ternary_seq.append(space_tok)\n",
    "    else:\n",
    "        print(f\"Word not found in panphon: {word_ipa}\")\n",
    "        #ternary_seq.append(space_tok)\n",
    "        #ternary_seq.append(np.zeros((1, emb_dim)))\n",
    "        continue\n",
    "np.concatenate(ternary_seq, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "dead4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "traits_list = [\"syl\", \"son\", \"cons\", \"cont\", \"delrel\", \"lat\",\n",
    "                \"nas\", \"strid\", \"voi\", \"sg\", \"cg\", \"ant\", \"cor\",\n",
    "                \"distr\", \"lab\", \"hi\", \"lo\", \"back\", \"round\",\n",
    "                \"velaric\", \"tense\", \"long\", \"hitone\", \"hireg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "06d9c336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  1, -1, -1, -1, -1, -1,  1, -1, -1,  1, -1,  0,  1, -1,\n",
       "        -1, -1, -1, -1,  0, -1,  0,  0]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.word_array(traits_list, u'bɝ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad35f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4798ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx in train_df.index[:1]:\n",
    "#    text = train_df.iloc[idx][\"text\"]\n",
    "#    text_to_arpabet(text, dictionary, cleaner_names=[\"english_cleaners_v2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53deec9",
   "metadata": {},
   "source": [
    "# Audio to art features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e95c0ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available train audio samples\n",
      "['LJ001-0006', 'LJ001-0008', 'LJ001-0009', 'LJ001-0005', 'LJ001-0001', 'LJ001-0002', 'LJ001-0007', 'LJ001-0004']\n"
     ]
    }
   ],
   "source": [
    "from sparc import load_model\n",
    "\n",
    "print(\"Available train audio samples\")\n",
    "train_audio_ids = list(set(train_df[\"id\"]).intersection(set([f\"LJ001-000{i}\" for i in range(1,10)])))\n",
    "print(train_audio_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "958044eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file /home/anli/Desktop/art-tts/src/../LJ_samples/LJ001-0006.wav exists\n"
     ]
    }
   ],
   "source": [
    "for id in train_audio_ids[:1]:\n",
    "    audio_path = data_dir / f\"{id}.wav\"\n",
    "    if not audio_path.exists():\n",
    "        print(f\"Audio file {audio_path} does not exist\")\n",
    "    else:\n",
    "        print(f\"Audio file {audio_path} exists\")\n",
    "        #play_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0178b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
