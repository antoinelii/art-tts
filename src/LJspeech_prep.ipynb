{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from utils import parse_filelist\n",
    "from text import cmudict\n",
    "from text.converters import text_to_arpabet, check_arpabet\n",
    "from paths import DATA_DIR\n",
    "\n",
    "data_dir = DATA_DIR / \"LJSpeech-1.1\"\n",
    "splits_dir = Path.cwd() / \"resources/filelists/ljspeech\"\n",
    "cmudict_path = 'resources/cmu_dictionary'\n",
    "\n",
    "dictionary = cmudict.CMUDict(cmudict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f22bd",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Create new filelists (arpabet convertible samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadat.csv\n",
    "filepaths_and_text = parse_filelist(data_dir / \"metadata.csv\", split_char='|')\n",
    "df = pd.DataFrame(np.array(filepaths_and_text), columns=[\"id\", \"transcript\", \"norm_transcript\"])\n",
    "for idx in range(10):\n",
    "    id = df.iloc[idx][\"id\"]\n",
    "    if df.loc[idx, \"transcript\"] != df.loc[idx, \"norm_transcript\"]:\n",
    "        print(f\"ID: {id}, idx: {idx}\")\n",
    "        print(f\"Original: {df.loc[idx, 'transcript']}\")\n",
    "        print(f\"Normalized: {df.loc[idx, 'norm_transcript']}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_df(filename:str=\"train.txt\"):\n",
    "    filepaths_and_text = parse_filelist(splits_dir / filename, split_char='|')\n",
    "    split_df = pd.DataFrame(np.array(filepaths_and_text), columns=[\"id\", \"text\"])\n",
    "    split_df[\"id\"] = split_df[\"id\"].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
    "    return split_df\n",
    "\n",
    "train_df = get_split_df(\"train.txt\")\n",
    "valid_df = get_split_df(\"valid.txt\")\n",
    "test_df = get_split_df(\"test.txt\")\n",
    "\n",
    "_ = train_df.merge(df, on=\"id\", how=\"left\")\n",
    "print(\"splits 'text' is metadata 'norm_transcript': \", np.all(_[\"norm_transcript\"] == _[\"text\"]))\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Validation samples: {len(valid_df)}, Test samples: {len(test_df)}\")\n",
    "print(f\"Train ratio: {len(train_df) / (len(train_df) + len(valid_df) + len(test_df)):.3f}, \"\n",
    "      f\"Validation ratio: {len(valid_df) / (len(train_df) + len(valid_df) + len(test_df)):.3f}, \"\n",
    "      f\"Test ratio: {len(test_df) / (len(train_df) + len(valid_df) + len(test_df)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_bad_df(split_df):\n",
    "    \"\"\"\n",
    "    Transcribe the samples from the dataframe to ARPabet\n",
    "    and return two dataframes:\n",
    "    1. good_samples_df: samples with valid ARPAbet\n",
    "    2. bad_samples_df: samples with invalid ARPAbet\n",
    "    \"\"\"\n",
    "    good_samples = []\n",
    "    bad_samples = []\n",
    "    for idx in split_df.index:\n",
    "        id = split_df.loc[idx][\"id\"]\n",
    "        text = split_df.loc[idx, \"text\"]\n",
    "        #text = \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n",
    "        cleaner_names=[\"english_cleaners_v2\"]\n",
    "        arpabets = text_to_arpabet(text, dictionary, cleaner_names)\n",
    "        arpabets = check_arpabet(arpabets, remove_punctuation=True)\n",
    "        if arpabets is None:\n",
    "            bad_samples.append({\"id\": id,\n",
    "                                \"text\": text,\n",
    "                                \"arpabets\": arpabets})\n",
    "        else:\n",
    "            good_samples.append({\"id\": id,\n",
    "                                \"text\": text,\n",
    "                                \"arpabets\": arpabets})\n",
    "    good_samples_df = pd.DataFrame(good_samples)\n",
    "    bad_samples_df = pd.DataFrame(bad_samples)\n",
    "    return good_samples_df, bad_samples_df\n",
    "\n",
    "train_good_df, train_bad_df = good_bad_df(train_df)\n",
    "valid_good_df, valid_bad_df = good_bad_df(valid_df)\n",
    "test_good_df, test_bad_df = good_bad_df(test_df)\n",
    "print(f\"Train good samples: {len(train_good_df)}, \\\n",
    "    Train conversion rate: {len(train_good_df) / len(train_df):.3f}\")\n",
    "print(f\"Validation good samples: {len(valid_good_df)}, \\\n",
    "    Validation conversion rate: {len(valid_good_df) / len(valid_df):.3f}\")\n",
    "print(f\"Test good samples: {len(test_good_df)}, \\\n",
    "    Test conversion rate: {len(test_good_df) / len(test_df):.3f}\")\n",
    "\n",
    "n_valid = (len(train_good_df) + len(valid_good_df) + len(test_good_df))\n",
    "print(f\"Train ratio: {len(train_good_df) / n_valid:.3f}, \"\n",
    "      f\"Validation ratio: {len(valid_good_df) / n_valid:.3f}, \"\n",
    "      f\"Test ratio: {len(test_good_df) / n_valid:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_good_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_split_file(filepath, splits_df):\n",
    "    lines = []\n",
    "    for row in splits_df[[\"id\", \"text\"]].values:\n",
    "        id = row[0]\n",
    "        text = row[1]\n",
    "        line = f\"DUMMY/{id}.wav|{text}\\n\"\n",
    "        lines.append(line)\n",
    "    with open(filepath, \"w\") as file:\n",
    "        file.writelines(lines)\n",
    "    print(f\"Filelist written to {filepath}\")\n",
    "\n",
    "\n",
    "#write_split_file(splits_dir / \"train_v0.txt\", train_good_df)\n",
    "#write_split_file(splits_dir / \"valid_v0.txt\", valid_good_df)\n",
    "#write_split_file(splits_dir / \"test_v0.txt\", test_good_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc15711",
   "metadata": {},
   "source": [
    "# ARPabet to IPA ternary traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37902f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_split_df(\"train_v0.txt\")\n",
    "valid_df = get_split_df(\"valid_v0.txt\")\n",
    "test_df = get_split_df(\"test_v0.txt\")\n",
    "print(f\"Train samples: {len(train_df)}, Validation samples: {len(valid_df)}, Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panphon\n",
    "from utils import intersperse\n",
    "from text.cleaners import _punctuation_list\n",
    "from text.converters import arpabet2ipa\n",
    "\n",
    "ft = panphon.FeatureTable()\n",
    "\n",
    "#'ɚ' and 'ɝ' were not recognized by panphon, we replaced them with 'ɜ˞' and 'ə˞' respectively\n",
    "# in the arpabet2ipa dictionary\n",
    "print(ft.validate_word('ɚ'))\n",
    "print(ft.validate_word('ɝ'))\n",
    "print(ft.validate_word('ɜ˞'))\n",
    "print(ft.validate_word('ə˞'))\n",
    "\n",
    "# Check if all ARPAbet symbols used in the dictionary are ipa translatable\n",
    "for k, v in arpabet2ipa.items():\n",
    "    if not ft.validate_word(v):\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text.converters import text_to_ipa, ipa_to_ternary\n",
    "from utils import intersperse\n",
    "\n",
    "add_blank = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'LJ001-0001'\n",
    "text = train_df.loc[train_df[\"id\"] == id, \"text\"].values[0]\n",
    "print(text)\n",
    "ipawords_list = text_to_ipa(text, dictionary, cleaner_names=[\"english_cleaners_v2\"], remove_punctuation=False)\n",
    "if add_blank:\n",
    "    ipawords_list = intersperse(ipawords_list, \" \")\n",
    "print(ipawords_list)\n",
    "ternary_emb = ipa_to_ternary(ipawords_list)\n",
    "print(ternary_emb.shape)\n",
    "print(ternary_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for id in train_df[\"id\"].values:\n",
    "#    text = train_df.loc[train_df[\"id\"] == id, \"text\"].values[0]\n",
    "#    ipawords_list = text_to_ipa(text, dictionary, cleaner_names=[\"english_cleaners_v2\"], remove_punctuation=False)\n",
    "#    if add_blank:\n",
    "#        ipawords_list = intersperse(ipawords_list, \" \")\n",
    "#    ternary_emb = ipa_to_ternary(ipawords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe169ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in valid_df[\"id\"].values:\n",
    "    text = valid_df.loc[valid_df[\"id\"] == id, \"text\"].values[0]\n",
    "    ipawords_list = text_to_ipa(text, dictionary, cleaner_names=[\"english_cleaners_v2\"], remove_punctuation=False)\n",
    "    if add_blank:\n",
    "        ipawords_list = intersperse(ipawords_list, \" \")\n",
    "    ternary_emb = ipa_to_ternary(ipawords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in test_df[\"id\"].values:\n",
    "    text = test_df.loc[test_df[\"id\"] == id, \"text\"].values[0]\n",
    "    ipawords_list = text_to_ipa(text, dictionary, cleaner_names=[\"english_cleaners_v2\"], remove_punctuation=False)\n",
    "    if add_blank:\n",
    "        ipawords_list = intersperse(ipawords_list, \" \")\n",
    "    ternary_emb = ipa_to_ternary(ipawords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53deec9",
   "metadata": {},
   "source": [
    "# Audio to art features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428dd9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sparc import load_model\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Since we don't have internet access on jean zay, \n",
    "# we download the model checkpoint from HuggingFace\n",
    "\n",
    "def download_huggingface(file_name):\n",
    "    return hf_hub_download(repo_id=\"cheoljun95/Speech-Articulatory-Coding\", filename=file_name,)\n",
    "\n",
    "model_name = \"model_english_1500k\"\n",
    "ckpt = download_huggingface(f\"{model_name}.ckpt\")\n",
    "ckpt = torch.load(ckpt)\n",
    "\n",
    "# Also need to download the speech model\n",
    "from transformers import WavLMModel\n",
    "\n",
    "speech_model = WavLMModel.from_pretrained(\"microsoft/wavlm-large\")\n",
    "#speech_model.save_pretrained(\"ckpt/wavlm-large\")\n",
    "\n",
    "# Modify default ckpt, so as to load the speech model from local path on jean zay\n",
    "ckpt[\"config\"][\"speech_model\"] = \"./ckpt/wavlm-large\"\n",
    "#torch.save(ckpt, \"ckpt/sparc_en.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c062dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparc import load_model\n",
    "from IPython.display import Audio\n",
    "\n",
    "coder = load_model(ckpt=\"ckpt/sparc_en.ckpt\", device= \"cpu\")\n",
    "#coder = load_model(\"feature_extraction\", device= \"cpu\")  # returns 1024 spk_emb..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_fp = data_dir / \"LJ001-0001.wav\"\n",
    "code = coder.encode(audio_fp, concat=True)\n",
    "#features = 12 EMA + pitch + loudness + periodicity\n",
    "for name, values in code.items():\n",
    "    print(f\"{name}: {values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356dfa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "from sparc import load_model\n",
    "\n",
    "from text import cmudict\n",
    "from text.converters import text_to_ipa, ipa_to_ternary\n",
    "from text.symbols import symbols\n",
    "from utils import parse_filelist, intersperse\n",
    "#from model.utils import fix_len_compatibility\n",
    "from configs.params_v0 import seed as random_seed\n",
    "from configs.params_v0 import (wavs_dir, artic_dir,\n",
    "                               sparc_ckpt_path)\n",
    "\n",
    "#import sys\n",
    "#sys.path.insert(0, 'hifi-gan')\n",
    "#from meldataset import mel_spectrogram\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spk_emb_save_dir = Path(artic_dir)/\"spk_emb\"\n",
    "spk_emb_save_dir.mkdir(exist_ok=True)\n",
    "ft_save_dir = Path(artic_dir)/\"emasrc\"\n",
    "ft_save_dir.mkdir(exist_ok=True)\n",
    "coder = load_model(ckpt=sparc_ckpt_path, \n",
    "                    device=device)\n",
    "\n",
    "class TextArticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filelist_path, cmudict_path, add_blank=True,\n",
    "                 sample_rate=22050,\n",
    "                 ):\n",
    "        self.filepaths_and_text = parse_filelist(filelist_path)\n",
    "        self.cmudict = cmudict.CMUDict(cmudict_path)\n",
    "        self.add_blank = add_blank\n",
    "        self.sample_rate = sample_rate\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(self.filepaths_and_text)\n",
    "\n",
    "    def get_pair(self,\n",
    "                 filepath_and_text:List[str],\n",
    "                 from_preprocessed:bool=True,\n",
    "                 )-> Tuple[torch.IntTensor, torch.FloatTensor]: # shape: (n_ipa_feats, seq_len), (n_art_feats, T)\n",
    "        filepath, text = filepath_and_text[0], filepath_and_text[1]\n",
    "        text = self.get_text(text, add_blank=self.add_blank)\n",
    "        art = self.get_art(filepath, from_preprocessed=from_preprocessed)\n",
    "        return (text, art)\n",
    "\n",
    "    def get_text(self,\n",
    "                 text:str,\n",
    "                 add_blank:bool=True\n",
    "                 )-> torch.IntTensor: # shape: (n_ipa_feats, seq_len)\n",
    "        ipawords_list = text_to_ipa(text, dictionary=self.cmudict, \n",
    "                                    cleaner_names=[\"english_cleaners_v2\"], \n",
    "                                    remove_punctuation=False)\n",
    "        if add_blank:\n",
    "            ipawords_list = intersperse(ipawords_list, \" \")\n",
    "        ternary_emb = ipa_to_ternary(ipawords_list)\n",
    "        ternary_emb = torch.IntTensor(ternary_emb).T  # shape: (n_ipa_feats, seq_len)\n",
    "        return ternary_emb\n",
    "    \n",
    "    def get_art(self,\n",
    "                filepath:str,\n",
    "                from_preprocessed:bool=True\n",
    "                )-> torch.FloatTensor: #shape: (n_art_feats, T)\n",
    "        art_filename = f\"{Path(filepath).stem}.npy\"\n",
    "        if from_preprocessed: # Favor loading precomputed features\n",
    "            preprocessed_fp = Path(artic_dir) / \"emasrc\" / art_filename\n",
    "            if preprocessed_fp.exists():\n",
    "                art = np.load(preprocessed_fp)[:, :14] # Extract only the first 14 articulatory features\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Preprocessed file {preprocessed_fp} does not exist.\")\n",
    "        else: # Long inference time better to precompute the features\n",
    "            filepath = filepath.replace(\"DUMMY/\", str(wavs_dir) + \"/\")\n",
    "            with torch.no_grad():\n",
    "                outputs = coder.encode(filepath, concat=True)\n",
    "            # Save the outputs to avoid recomputing\n",
    "            if not ft_save_dir.exists():\n",
    "                ft_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            if not spk_emb_save_dir.exists():\n",
    "                spk_emb_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            ft_save_path = ft_save_dir/art_filename\n",
    "            spk_emb_save_path = spk_emb_save_dir/art_filename\n",
    "            np.save(ft_save_path, outputs[\"features\"])\n",
    "            np.save(spk_emb_save_path, outputs[\"spk_emb\"])\n",
    "            # Extract the first 14 features\n",
    "            art = outputs[\"features\"][:, :14]\n",
    "        return torch.FloatTensor(art).T # shape: (n_art_feats, T)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text, art = self.get_pair(self.filepaths_and_text[index], from_preprocessed=True)\n",
    "        item = {'y': art, 'x': text}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths_and_text)\n",
    "\n",
    "    def sample_test_batch(self, size):\n",
    "        idx = np.random.choice(range(len(self)), size=size, replace=False)\n",
    "        test_batch = []\n",
    "        for index in idx:\n",
    "            test_batch.append(self.__getitem__(index))\n",
    "        return test_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3894ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.params_v0 import (cmudict_path,\n",
    "                               train_filelist_path,\n",
    "                               valid_filelist_path, test_filelist_path)\n",
    "\n",
    "train_dataset = TextArticDataset(\n",
    "    filelist_path=train_filelist_path,\n",
    "    cmudict_path=cmudict_path,\n",
    "    add_blank=True,\n",
    ")\n",
    "valid_dataset = TextArticDataset(\n",
    "    filelist_path=valid_filelist_path,\n",
    "    cmudict_path=cmudict_path,\n",
    "    add_blank=True,\n",
    ")\n",
    "test_dataset = TextArticDataset(\n",
    "    filelist_path=test_filelist_path,\n",
    "    cmudict_path=cmudict_path,\n",
    "    add_blank=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(train_dataset.filepaths_and_text):\n",
    "    if \"LJ001-0001.wav\" in e[0]:\n",
    "        print(\"found\")\n",
    "        fp_and_txt = e\n",
    "        idx = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad89809",
   "metadata": {},
   "outputs": [],
   "source": [
    "(text, art) = train_dataset.get_pair(fp_and_txt, from_preprocessed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd890173",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.shape, art.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18eb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_dataset[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import fix_len_compatibility\n",
    "\n",
    "B = len(batch)\n",
    "y_max_length = max([item['y'].shape[-1] for item in batch])\n",
    "y_max_length = fix_len_compatibility(y_max_length)\n",
    "x_max_length = max([item['x'].shape[-1] for item in batch])\n",
    "n_feats = batch[0]['y'].shape[-2]\n",
    "n_ipa_feats = batch[0]['x'].shape[-2]\n",
    "\n",
    "y = torch.zeros((B, n_feats, y_max_length), dtype=torch.float32)\n",
    "x = torch.zeros((B, n_ipa_feats, x_max_length), dtype=torch.long)\n",
    "y_lengths, x_lengths = [], []\n",
    "\n",
    "for i, item in enumerate(batch):\n",
    "    y_, x_ = item['y'], item['x']\n",
    "    y_lengths.append(y_.shape[-1])\n",
    "    x_lengths.append(x_.shape[-1])\n",
    "    y[i, :, :y_.shape[-1]] = y_\n",
    "    x[i, :, :x_.shape[-1]] = x_\n",
    "\n",
    "y_lengths = torch.LongTensor(y_lengths)\n",
    "x_lengths = torch.LongTensor(x_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lengths, y_lengths, x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea22233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import fix_len_compatibility\n",
    "\n",
    "class TextArticBatchCollate(object):\n",
    "    def __call__(self, batch):\n",
    "        B = len(batch)\n",
    "        y_max_length = max([item['y'].shape[-1] for item in batch])\n",
    "        y_max_length = fix_len_compatibility(y_max_length)\n",
    "        x_max_length = max([item['x'].shape[-1] for item in batch])\n",
    "        n_feats = batch[0]['y'].shape[-2]\n",
    "\n",
    "        y = torch.zeros((B, n_feats, y_max_length), dtype=torch.float32)\n",
    "        x = torch.zeros((B, x_max_length), dtype=torch.long)\n",
    "        y_lengths, x_lengths = [], []\n",
    "\n",
    "        for i, item in enumerate(batch):\n",
    "            y_, x_ = item['y'], item['x']\n",
    "            y_lengths.append(y_.shape[-1])\n",
    "            x_lengths.append(x_.shape[-1])\n",
    "            y[i, :, :y_.shape[-1]] = y_\n",
    "            x[i, :x_.shape[-1]] = x_\n",
    "\n",
    "        y_lengths = torch.LongTensor(y_lengths)\n",
    "        x_lengths = torch.LongTensor(x_lengths)\n",
    "        return {'x': x, 'x_lengths': x_lengths, 'y': y, 'y_lengths': y_lengths}\n",
    "    \n",
    "batch_collate = TextArticBatchCollate()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                        collate_fn=batch_collate, drop_last=True,\n",
    "                        num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204b0ed",
   "metadata": {},
   "source": [
    "# Create v4 filelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filestem(filepath:str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the filestem from the filepath.\n",
    "    \"\"\"\n",
    "    return filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "def art_filepath(filestem:str, prefix:str) -> str:\n",
    "    return prefix + f\"{filestem}.npy\"\n",
    "\n",
    "def write_filelist(filepath, filelist, sep='|'):\n",
    "    lines = []\n",
    "    for e in filelist:\n",
    "        lines.append(f\"{e[0]}{sep}{e[1]}\\n\")\n",
    "    with open(filepath, \"w\") as file:\n",
    "        file.writelines(lines)\n",
    "    print(f\"Filelist written to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LJSpeech dataset\n",
    "from paths import FILELISTS_DIR\n",
    "from utils import parse_filelist\n",
    "\n",
    "dataset = \"ljspeech\"\n",
    "DATASET = \"LJSpeech-1.1\"\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    v0_filelist_path = FILELISTS_DIR / f\"{dataset}/{split}_v0.txt\"\n",
    "    v0_filelist = parse_filelist(v0_filelist_path, split_char='|')\n",
    "    art_prefix = f\"DUMMY/{DATASET}/encoded_audio_en/emasrc/\"\n",
    "    v4_filelist = [[f\"{art_filepath(get_filestem(fp), art_prefix)}\", text] for fp, text in v0_filelist]\n",
    "    v4_filelist_path = FILELISTS_DIR / f\"{dataset}/{split}_v4.txt\"\n",
    "    #write_filelist(v4_filelist_path, v4_filelist, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9479ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_dataset.mngu0 import get_mngu0_sentence\n",
    "from pathlib import Path\n",
    "from paths import DATA_DIR, FILELISTS_DIR\n",
    "from utils import parse_filelist\n",
    "\n",
    "dataset = \"MNGU0\"\n",
    "speakers=[\"s1\"]\n",
    "\n",
    "for spk in speakers:\n",
    "    sentence_dir = DATA_DIR / dataset / \"src_data\" / spk / \"phone_labels\"\n",
    "    v1_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v1.txt\"\n",
    "    v1_filelist = parse_filelist(v1_filelist_path, split_char='|')\n",
    "    filelist = []\n",
    "    for e in v1_filelist:\n",
    "        wav_fp = e[0]\n",
    "        filestem = get_filestem(wav_fp)\n",
    "        sentence = get_mngu0_sentence(sentence_dir / f\"{filestem}.utt\")\n",
    "        ema_fp = f\"DUMMY/{dataset}/arttts/{spk}/encoded_audio_en/emasrc/{filestem}.npy\"\n",
    "        filelist.append([ema_fp, sentence])\n",
    "    v4_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v4.txt\"\n",
    "    #write_filelist(v4_filelist_path, filelist, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import utils_ema.ema_dataset\n",
    "from pathlib import Path\n",
    "from paths import DATA_DIR, FILELISTS_DIR\n",
    "from utils import parse_filelist\n",
    "\n",
    "dataset = \"mocha_timit\"\n",
    "speakers=[\"faet0\", \"ffes0\", \"fsew0\", \"maps0\", \"mjjn0\", \"msak0\"]\n",
    "processed_data_dir = DATA_DIR / dataset / \"processed_data\"\n",
    "spkmetadata_filename = \"mixed_speaker_metadata_100Hz.joblib\"\n",
    "\n",
    "\n",
    "for spk in speakers:\n",
    "    #get sentences dict\n",
    "    spkmeta = joblib.load(processed_data_dir / f\"{spk}/{spkmetadata_filename}\")\n",
    "    ids = spkmeta.list_valid_ids()\n",
    "    sentences_dict = {}\n",
    "    for id in ids:\n",
    "        sentencemeta = spkmeta.sentence_info[id]\n",
    "        sentences_dict[sentencemeta.filestem] = sentencemeta.sentence\n",
    "    #get filelist samples\n",
    "    v1_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v1.txt\"\n",
    "    v1_filelist = parse_filelist(v1_filelist_path, split_char='|')\n",
    "    filelist = []\n",
    "    for e in v1_filelist:\n",
    "        wav_fp = e[0]\n",
    "        filestem = get_filestem(wav_fp)\n",
    "        ema_fp = f\"DUMMY/{dataset}/arttts/{spk}/encoded_audio_en/emasrc/{filestem}.npy\"\n",
    "        sentence = sentences_dict.get(filestem, \"No sentence found\")\n",
    "        filelist.append([ema_fp, sentence])\n",
    "    v4_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v4.txt\"\n",
    "    #write_filelist(v4_filelist_path, filelist, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import utils_ema.ema_dataset\n",
    "from pathlib import Path\n",
    "from paths import DATA_DIR, FILELISTS_DIR\n",
    "from utils import parse_filelist\n",
    "\n",
    "dataset = \"MSPKA_EMA_ita\"\n",
    "speakers=[\"cnz\", \"lls\", \"olm\"]\n",
    "processed_data_dir = DATA_DIR / dataset / \"processed_data\"\n",
    "spkmetadata_filename = \"mixed_speaker_metadata_100Hz.joblib\"\n",
    "\n",
    "for spk in speakers:\n",
    "    #get sentences dict\n",
    "    spkmeta = joblib.load(processed_data_dir / f\"{spk}/{spkmetadata_filename}\")\n",
    "    ids = spkmeta.list_valid_ids()\n",
    "    sentences_dict = {}\n",
    "    for id in ids:\n",
    "        sentencemeta = spkmeta.sentence_info[id]\n",
    "        sentences_dict[sentencemeta.filestem] = sentencemeta.sentence\n",
    "    #get filelist samples\n",
    "    v1_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v1.txt\"\n",
    "    v1_filelist = parse_filelist(v1_filelist_path, split_char='|')\n",
    "    filelist = []\n",
    "    for e in v1_filelist:\n",
    "        wav_fp = e[0]\n",
    "        filestem = get_filestem(wav_fp)\n",
    "        ema_fp = f\"DUMMY/{dataset}/arttts/{spk}/encoded_audio_en/emasrc/{filestem}.npy\"\n",
    "        sentence = sentences_dict.get(filestem, \"No sentence found\")\n",
    "        filelist.append([ema_fp, sentence])\n",
    "    v4_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v4.txt\"\n",
    "    #write_filelist(v4_filelist_path, filelist, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd13876",
   "metadata": {},
   "source": [
    "# Create v2 filelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filestem(filepath:str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the filestem from the filepath.\n",
    "    \"\"\"\n",
    "    return filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "def wav_filepath(filestem:str, prefix:str) -> str:\n",
    "    return prefix + f\"{filestem}.wav\"\n",
    "\n",
    "def write_filelist(filepath, filelist, sep='|'):\n",
    "    lines = []\n",
    "    for e in filelist:\n",
    "        lines.append(f\"{e[0]}{sep}{e[1]}\\n\")\n",
    "    with open(filepath, \"w\") as file:\n",
    "        file.writelines(lines)\n",
    "    print(f\"Filelist written to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f95a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LJSpeech dataset\n",
    "from paths import FILELISTS_DIR\n",
    "from utils import parse_filelist\n",
    "\n",
    "dataset = \"ljspeech\"\n",
    "DATASET = \"LJSpeech-1.1\"\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    v0_filelist_path = FILELISTS_DIR / f\"{dataset}/{split}_v0.txt\"\n",
    "    v0_filelist = parse_filelist(v0_filelist_path, split_char='|')\n",
    "    wav_prefix = f\"DUMMY/{DATASET}/wavs/\"\n",
    "    v2_filelist = [[f\"{wav_filepath(get_filestem(fp), wav_prefix)}\", text] for fp, text in v0_filelist]\n",
    "    v2_filelist_path = FILELISTS_DIR / f\"{dataset}/{split}_v2.txt\"\n",
    "    #write_filelist(v2_filelist_path, v2_filelist, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LJSpeech dataset\n",
    "from paths import FILELISTS_DIR\n",
    "from utils import parse_filelist\n",
    "\n",
    "dataset = \"ljspeech\"\n",
    "DATASET = \"LJSpeech-1.1\"\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    v0_filelist_path = FILELISTS_DIR / f\"{dataset}/{split}.txt\"\n",
    "    v0_filelist = parse_filelist(v0_filelist_path, split_char='|')\n",
    "    wav_prefix = f\"DUMMY/{DATASET}/wavs/\"\n",
    "    v2_filelist = [[f\"{wav_filepath(get_filestem(fp), wav_prefix)}\", text] for fp, text in v0_filelist]\n",
    "    v2_filelist_path = FILELISTS_DIR / f\"{dataset}/{split}_v2_full.txt\"\n",
    "    #write_filelist(v2_filelist_path, v2_filelist, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_dataset.mngu0 import get_mngu0_sentence\n",
    "from pathlib import Path\n",
    "from paths import DATA_DIR, FILELISTS_DIR\n",
    "from utils import parse_filelist\n",
    "\n",
    "dataset = \"MNGU0\"\n",
    "speakers=[\"s1\"]\n",
    "\n",
    "for spk in speakers:\n",
    "    sentence_dir = DATA_DIR / dataset / \"src_data\" / spk / \"phone_labels\"\n",
    "    v1_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v1.txt\"\n",
    "    v1_filelist = parse_filelist(v1_filelist_path, split_char='|')\n",
    "    filelist = []\n",
    "    for e in v1_filelist:\n",
    "        wav_fp = e[0]\n",
    "        filestem = get_filestem(wav_fp)\n",
    "        sentence = get_mngu0_sentence(sentence_dir / f\"{filestem}.utt\")\n",
    "        prefix = f\"DUMMY/{dataset}/src_data/{spk}/wav_16kHz/\"\n",
    "        wav_fp = wav_filepath(filestem, prefix)\n",
    "        filelist.append([wav_fp, sentence])\n",
    "    v2_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v2.txt\"\n",
    "    write_filelist(v2_filelist_path, filelist, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd916271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import utils_ema.ema_dataset\n",
    "from pathlib import Path\n",
    "from paths import DATA_DIR, FILELISTS_DIR\n",
    "from utils import parse_filelist\n",
    "\n",
    "dataset = \"mocha_timit\"\n",
    "speakers=[\"faet0\", \"ffes0\", \"fsew0\", \"maps0\", \"mjjn0\", \"msak0\"]\n",
    "processed_data_dir = DATA_DIR / dataset / \"processed_data\"\n",
    "spkmetadata_filename = \"mixed_speaker_metadata_100Hz.joblib\"\n",
    "\n",
    "\n",
    "for spk in speakers:\n",
    "    #get sentences dict\n",
    "    spkmeta = joblib.load(processed_data_dir / f\"{spk}/{spkmetadata_filename}\")\n",
    "    ids = spkmeta.list_valid_ids()\n",
    "    sentences_dict = {}\n",
    "    for id in ids:\n",
    "        sentencemeta = spkmeta.sentence_info[id]\n",
    "        sentences_dict[sentencemeta.filestem] = sentencemeta.sentence\n",
    "    #get filelist samples\n",
    "    v1_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v1.txt\"\n",
    "    v1_filelist = parse_filelist(v1_filelist_path, split_char='|')\n",
    "    filelist = []\n",
    "    for e in v1_filelist:\n",
    "        wav_fp = e[0]\n",
    "        filestem = get_filestem(wav_fp)\n",
    "        sentence = sentences_dict.get(filestem, \"No sentence found\")\n",
    "        prefix = f\"DUMMY/{dataset}/src_data/wavs/\"\n",
    "        wav_fp = wav_filepath(filestem, prefix)\n",
    "        filelist.append([wav_fp, sentence])\n",
    "    v2_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v2.txt\"\n",
    "    #write_filelist(v2_filelist_path, filelist, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299bbe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import utils_ema.ema_dataset\n",
    "from pathlib import Path\n",
    "from paths import DATA_DIR, FILELISTS_DIR\n",
    "from utils import parse_filelist\n",
    "\n",
    "dataset = \"MSPKA_EMA_ita\"\n",
    "speakers=[\"cnz\", \"lls\", \"olm\"]\n",
    "processed_data_dir = DATA_DIR / dataset / \"processed_data\"\n",
    "spkmetadata_filename = \"mixed_speaker_metadata_100Hz.joblib\"\n",
    "\n",
    "for spk in speakers:\n",
    "    #get sentences dict\n",
    "    spkmeta = joblib.load(processed_data_dir / f\"{spk}/{spkmetadata_filename}\")\n",
    "    ids = spkmeta.list_valid_ids()\n",
    "    sentences_dict = {}\n",
    "    for id in ids:\n",
    "        sentencemeta = spkmeta.sentence_info[id]\n",
    "        sentences_dict[sentencemeta.filestem] = sentencemeta.sentence\n",
    "    #get filelist samples\n",
    "    v1_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v1.txt\"\n",
    "    v1_filelist = parse_filelist(v1_filelist_path, split_char='|')\n",
    "    filelist = []\n",
    "    for e in v1_filelist:\n",
    "        wav_fp = e[0]\n",
    "        filestem = get_filestem(wav_fp)\n",
    "        sentence = sentences_dict.get(filestem, \"No sentence found\")        \n",
    "        prefix = f\"DUMMY/{dataset}/src_data/{spk}/\"\n",
    "        wav_fp = wav_filepath(filestem, prefix)\n",
    "        filelist.append([wav_fp, sentence])\n",
    "    v2_filelist_path = FILELISTS_DIR / f\"{dataset}/{spk}_v2.txt\"\n",
    "    #write_filelist(v2_filelist_path, filelist, sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
