{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c1eda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from paths import DATA_DIR\n",
    "from voxcommunis.io import read_alignment, read_manifest\n",
    "from voxcommunis.data import (PanPhonInventory, FeatureTokenizer, PhoneticFeatureDataset,\n",
    "                              SAMPLE_RATE, ALIGNMENT_FREQ, MODEL_FREQ, SUBSAMPLE\n",
    ")\n",
    "from voxcommunis.decoder import UniqueSegmentFeature, UniversalUniqueSegmentFeature, FeatureDecoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "vc_dir = Path(DATA_DIR) / \"VoxCommunis\"\n",
    "\n",
    "split = \"train\"\n",
    "alignments_dir = vc_dir / split / \"alignments\"\n",
    "manifests_dir = vc_dir / split / \"manifests\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ae5b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submanifest(lang, src_filepath, new_filepath, max_duration=3600):\n",
    "    manifest = read_manifest(src_filepath)\n",
    "    # Get the prefix from the first line of the manifest file\n",
    "    with open(src_filepath) as f:\n",
    "        lines = f.readlines()\n",
    "    prefix = Path(lines[0].strip().split(\"\\t\")[0])\n",
    "    # Initialize with the prefix line\n",
    "    new_lines = [str(prefix) + '\\n']\n",
    "\n",
    "    #shuffle the samples\n",
    "    sample_ids = list(manifest.keys())\n",
    "    random.shuffle(sample_ids)\n",
    "\n",
    "    # get 1h of samples if possible\n",
    "    tot_duration = 0\n",
    "    while (tot_duration < max_duration) and sample_ids:\n",
    "        sample_id = sample_ids.pop()\n",
    "        full_path, frames = manifest[sample_id]\n",
    "        tot_duration += int(frames) / SAMPLE_RATE # convert frames to seconds\n",
    "        new_line = str(full_path.relative_to(prefix)) + f\"\\t{frames}\\n\"\n",
    "        new_lines.append(new_line)\n",
    "    print(f\"For lang {lang}, {len(new_lines)} samples, total duration: {int(tot_duration // 3600)} h {int((tot_duration % 3600) // 60)} min\")\n",
    "    # Write the new manifest file\n",
    "    with open(new_filepath, \"w\") as f:\n",
    "        f.writelines(new_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbf370",
   "metadata": {},
   "source": [
    "# Create multilingual 1h/lang manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b5fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "src_manifests_dir = vc_dir / split / \"manifests\"\n",
    "langs = list(set(e.stem for e in list(src_manifests_dir.glob(\"*.tsv\"))))\n",
    "\n",
    "dest_manifests_dir = vc_dir / \"train-1h\" / \"manifests\"\n",
    "dest_manifests_dir.mkdir(exist_ok=True, parents=True)\n",
    "max_duration = 3600  # 1 hour in seconds\n",
    "\n",
    "for lang in langs:\n",
    "    src_filepath = src_manifests_dir / f\"{lang}.tsv\"\n",
    "    new_filepath = dest_manifests_dir / f\"{lang}.tsv\"\n",
    "    #create_submanifest(lang, src_filepath, new_filepath, max_duration=max_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a80c0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"dev\"\n",
    "src_manifests_dir = vc_dir / split / \"manifests\"\n",
    "langs = list(set(e.stem for e in list(src_manifests_dir.glob(\"*.tsv\"))))\n",
    "\n",
    "dest_manifests_dir = vc_dir / \"dev-1h\" / \"manifests\"\n",
    "dest_manifests_dir.mkdir(exist_ok=True, parents=True)\n",
    "max_duration = 1200  # 20 minutes\n",
    "\n",
    "for lang in langs:\n",
    "    src_filepath = src_manifests_dir / f\"{lang}.tsv\"\n",
    "    new_filepath = dest_manifests_dir / f\"{lang}.tsv\"\n",
    "    #create_submanifest(lang, src_filepath, new_filepath,\n",
    "    #                    max_duration=max_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e08000ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "src_manifests_dir = vc_dir / split / \"manifests\"\n",
    "langs = list(set(e.stem for e in list(src_manifests_dir.glob(\"*.tsv\"))))\n",
    "\n",
    "dest_manifests_dir = vc_dir / \"test-1h\" / \"manifests\"\n",
    "dest_manifests_dir.mkdir(exist_ok=True, parents=True)\n",
    "max_duration = 1200  # 20 minutes\n",
    "\n",
    "for lang in langs:\n",
    "    src_filepath = src_manifests_dir / f\"{lang}.tsv\"\n",
    "    new_filepath = dest_manifests_dir / f\"{lang}.tsv\"\n",
    "    #create_submanifest(lang, src_filepath, new_filepath,\n",
    "    #                    max_duration=max_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6ad20",
   "metadata": {},
   "source": [
    "# Create monolingual 20h manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "src_manifests_dir = vc_dir / split / \"manifests\"\n",
    "\n",
    "dest_manifests_dir = vc_dir / \"train-20h\" / \"manifests\"\n",
    "dest_manifests_dir.mkdir(exist_ok=True, parents=True)\n",
    "max_duration = 20 * 3600  # 20 hour in seconds\n",
    "\n",
    "langs = ['fr', 'eu', 'it', 'sw', 'hu', 'zh-CN', 'ru']\n",
    "for lang in langs:\n",
    "    src_filepath = src_manifests_dir / f\"{lang}.tsv\"\n",
    "    new_filepath = dest_manifests_dir / f\"{lang}.tsv\"\n",
    "    #create_submanifest(lang, src_filepath, new_filepath, max_duration=max_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b10041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lang fr, 1240 samples, total duration: 2 h 0 min\n",
      "For lang eu, 1223 samples, total duration: 2 h 0 min\n",
      "For lang it, 1269 samples, total duration: 2 h 0 min\n",
      "For lang sw, 1284 samples, total duration: 2 h 0 min\n",
      "For lang hu, 1363 samples, total duration: 2 h 0 min\n",
      "For lang zh-CN, 1331 samples, total duration: 2 h 0 min\n",
      "For lang ru, 1337 samples, total duration: 2 h 0 min\n"
     ]
    }
   ],
   "source": [
    "split = \"dev\"\n",
    "src_manifests_dir = vc_dir / split / \"manifests\"\n",
    "\n",
    "dest_manifests_dir = vc_dir / \"dev-20h\" / \"manifests\"\n",
    "dest_manifests_dir.mkdir(exist_ok=True, parents=True)\n",
    "max_duration = 2 * 3600  # 2 hour in seconds\n",
    "\n",
    "langs = ['fr', 'eu', 'it', 'sw', 'hu', 'zh-CN', 'ru']\n",
    "for lang in langs:\n",
    "    src_filepath = src_manifests_dir / f\"{lang}.tsv\"\n",
    "    new_filepath = dest_manifests_dir / f\"{lang}.tsv\"\n",
    "    #create_submanifest(lang, src_filepath, new_filepath,\n",
    "    #                   max_duration=max_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3ec1c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lang fr, 1238 samples, total duration: 2 h 0 min\n",
      "For lang eu, 1184 samples, total duration: 2 h 0 min\n",
      "For lang it, 1200 samples, total duration: 2 h 0 min\n",
      "For lang sw, 1302 samples, total duration: 2 h 0 min\n",
      "For lang hu, 1291 samples, total duration: 2 h 0 min\n",
      "For lang zh-CN, 1216 samples, total duration: 2 h 0 min\n",
      "For lang ru, 1302 samples, total duration: 2 h 0 min\n"
     ]
    }
   ],
   "source": [
    "split = \"test\"\n",
    "src_manifests_dir = vc_dir / split / \"manifests\"\n",
    "\n",
    "dest_manifests_dir = vc_dir / \"test-20h\" / \"manifests\"\n",
    "dest_manifests_dir.mkdir(exist_ok=True, parents=True)\n",
    "max_duration = 2 * 3600  # 20 hour in seconds\n",
    "\n",
    "langs = ['fr', 'eu', 'it', 'sw', 'hu', 'zh-CN', 'ru']\n",
    "for lang in langs:\n",
    "    src_filepath = src_manifests_dir / f\"{lang}.tsv\"\n",
    "    new_filepath = dest_manifests_dir / f\"{lang}.tsv\"\n",
    "    create_submanifest(lang, src_filepath, new_filepath,\n",
    "                                max_duration=max_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01055f06",
   "metadata": {},
   "source": [
    "# Create alignments for the new manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdbc9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subalignments(manifest_fp, src_ali_fp, dest_ali_fp):\n",
    "    manifest = read_manifest(manifest_fp)\n",
    "    alignment = read_alignment(src_ali_fp)\n",
    "    new_lines = []\n",
    "    for sample_id in manifest.keys():\n",
    "        if sample_id in alignment:\n",
    "            new_line = f\"{sample_id}\\t\"\n",
    "            new_line += ' '.join(alignment[sample_id].split(' ')) + '\\n'\n",
    "            new_lines.append(new_line)\n",
    "        else:\n",
    "            print(f\"Warning: {sample_id} not in alignment file {src_ali_fp}\")\n",
    "            continue\n",
    "        \n",
    "    with open(dest_ali_fp, \"w\") as f:\n",
    "        f.writelines(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2b4f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "manifests_dir = vc_dir / \"test-1h\" / \"manifests\"\n",
    "alignments_dir = vc_dir / \"test-1h\" / \"alignments\"\n",
    "os.makedirs(alignments_dir, exist_ok=True)\n",
    "src_alignments_dir = vc_dir / \"test\" / \"alignments\"\n",
    "for man in manifests_dir.glob(\"*.tsv\"):\n",
    "    lang = man.stem\n",
    "    #print(f\"Processing language: {lang}\")\n",
    "    #create_subalignments(manifests_dir / f\"{lang}.tsv\",\n",
    "    #                     src_alignments_dir / f\"{lang}.align\",\n",
    "    #                     alignments_dir / f\"{lang}.align\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d674cd",
   "metadata": {},
   "source": [
    "# Global info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "src_manifests_dir = vc_dir / split / \"manifests\"\n",
    "langs = list(set(e.stem for e in list(src_manifests_dir.glob(\"*.tsv\"))))\n",
    "\n",
    "durations = []\n",
    "for lang in langs:\n",
    "    manifest = read_manifest(src_manifests_dir / f\"{lang}.tsv\")\n",
    "    sample_ids = list(manifest.keys())\n",
    "\n",
    "    tot_duration = 0\n",
    "    while sample_ids:\n",
    "        sample_id = sample_ids.pop()\n",
    "        full_path, frames = manifest[sample_id]\n",
    "        tot_duration += int(frames) / SAMPLE_RATE # convert frames to seconds\n",
    "    durations.append(tot_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_df = pd.DataFrame(np.array([langs, durations]).T, columns=[\"lang\", \"duration\"])\n",
    "hours_df[\"duration\"] = hours_df[\"duration\"].astype(float) / 3600  # convert to hours\n",
    "hours_df.sort_values(by=\"duration\", ascending=False, inplace=True)\n",
    "hours_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manifest_df(fp):\n",
    "    with open(fp, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    dataset_prefix = lines[0]\n",
    "    lines = [line.split(\"\\t\") for line in lines[1:]]\n",
    "    manifest_df = pd.DataFrame(lines, columns=[\"wav_fp\", \"frames\"])\n",
    "    manifest_df[\"sample_id\"] = manifest_df[\"wav_fp\"].apply(lambda x: Path(x).stem)\n",
    "    manifest_df[\"wav_fp\"] = manifest_df[\"wav_fp\"].apply(lambda x: dataset_prefix + '/' + x)\n",
    "    manifest_df = manifest_df[[\"sample_id\", \"wav_fp\", \"frames\"]]\n",
    "    return manifest_df\n",
    "\n",
    "manifest_df = get_manifest_df(manifests_dir / f\"{lang}.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d1d3e",
   "metadata": {},
   "source": [
    "## Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from panphon import FeatureTable\n",
    "from text.converters import traits_list\n",
    "\n",
    "ft = FeatureTable()\n",
    "raw_inventories_dir = vc_dir / \"raw_inventories\"\n",
    "clean_inventories_dir = vc_dir / \"clean_inventories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27511ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_superset = set()\n",
    "langs = list(set(e.stem for e in list(raw_inventories_dir.glob(\"*.pickle\"))))\n",
    "for lang in langs:\n",
    "    raw_inventory = pickle.load(open(raw_inventories_dir / f\"{lang}.pickle\", \"rb\"))\n",
    "    clean_inventory = pickle.load(open(clean_inventories_dir / f\"{lang}.pickle\", \"rb\"))\n",
    "    phoneme_superset.update(clean_inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(phoneme_superset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae53f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "panphon_inventory = PanPhonInventory()\n",
    "\n",
    "for e in list(raw_inventory):\n",
    "    if e != \"SIL\":\n",
    "        e_bis = panphon_inventory.convert_to_ipa(e)[0]\n",
    "        assert ft.word_array(traits_list, e_bis).shape[0] == 1, f\"Error in {e} -> {e_bis}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af878844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panphon\n",
    "\n",
    "ft = panphon.FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadd4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ft.segments[0][1].numeric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ft.segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf98a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "uusf = UniversalUniqueSegmentFeature(sum_diphthong=True)\n",
    "len(uusf.unique_segments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa11112",
   "metadata": {},
   "source": [
    "## Phoneme Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708c687",
   "metadata": {},
   "source": [
    "# Manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11f45b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from voxcommunis.io import read_manifest, read_alignment\n",
    "from paths import DATA_DIR\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from voxcommunis.decoder import (UniversalUniqueSegmentFeature,\n",
    "                                 UniqueSegmentFeature,\n",
    "                                 FeatureDecoder\n",
    "                                )\n",
    "from voxcommunis.data import FeatureTokenizer, PhoneticFeatureDataset, SUBSAMPLE, PanPhonInventory\n",
    "from voxcommunis.utils import unique_consecutive\n",
    "\n",
    "\n",
    "vc_dir = Path(DATA_DIR) / \"VoxCommunis\"\n",
    "\n",
    "lang = \"it\"\n",
    "manifest_fp = vc_dir / \"train-20h\" / \"manifests\" / f\"{lang}.tsv\"\n",
    "alignment_fp = vc_dir / \"train\" / \"alignments\" / f\"{lang}.align\"\n",
    "encoded_dir = vc_dir / \"encoded_audio_multi\" / lang\n",
    "emasrc_dir = encoded_dir / \"emasrc\"\n",
    "spk_emb_dir = encoded_dir / \"spk_preemb\"\n",
    "\n",
    "manifest = read_manifest(manifest_fp)\n",
    "alignment = read_alignment(alignment_fp)\n",
    "\n",
    "\n",
    "uusf = UniversalUniqueSegmentFeature(sum_diphthong=True)\n",
    "usf = UniqueSegmentFeature(uusf.unique_segments, sum_diphthong=True)\n",
    "fd = FeatureDecoder(sum_diphthong=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = list(manifest.keys())\n",
    "sample_id = sample_ids[0]\n",
    "\n",
    "emasrc = np.load(emasrc_dir / f\"{sample_id}.npy\")[:,:14]\n",
    "spk_preemb = np.load(spk_emb_dir / f\"{sample_id}.npy\")\n",
    "segments = alignment[sample_id].split(\" \")\n",
    "\n",
    "#process phonemes\n",
    "tokenizer = FeatureTokenizer(fd)\n",
    "segments_sub = segments[::SUBSAMPLE]\n",
    "ipa_phones, counts = unique_consecutive(segments_sub, return_counts=True)\n",
    "feature_tensor, phones = tokenizer.encode(ipa_phones, counts)\n",
    "sil_trait = (feature_tensor == 0).all(axis=1) * 2 - 1 # 1 for sil, -1 for non-sil\n",
    "feature_tensor25 = torch.concat([feature_tensor, sil_trait.unsqueeze(1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bffcaf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 26]) 61\n"
     ]
    }
   ],
   "source": [
    "manifests_dir = vc_dir / \"train-20h\" / \"manifests\"\n",
    "alignments_dir = vc_dir / \"train-20h\" / \"alignments\"\n",
    "\n",
    "truc = PhoneticFeatureDataset(\n",
    "    manifest_path=manifests_dir,\n",
    "    alignment_path=alignments_dir,\n",
    "    feature_tokenizer=tokenizer,\n",
    "    separate_files=True,\n",
    ")\n",
    "phon_features, phones = truc[0]\n",
    "print(phon_features.shape, len(phones))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
